{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RNN.ipynb","version":"0.3.2","provenance":[{"file_id":"1lTuWEtA_U8zifCRM-r6pntbn7ffORbUZ","timestamp":1537840266153}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"n8qXic6srJuq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"613ced0a-224e-41bf-81ef-f1f652928faf","executionInfo":{"status":"ok","timestamp":1537844489461,"user_tz":-480,"elapsed":2616,"user":{"displayName":"sara z","photoUrl":"","userId":"01951022893791472695"}}},"cell_type":"code","source":["!ls"],"execution_count":1,"outputs":[{"output_type":"stream","text":["sample_data\n"],"name":"stdout"}]},{"metadata":{"id":"IswVPsyPZxfL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"c71fcb71-b19f-4b0a-e53f-c9486bf8254f","executionInfo":{"status":"ok","timestamp":1537844512236,"user_tz":-480,"elapsed":21211,"user":{"displayName":"sara z","photoUrl":"","userId":"01951022893791472695"}}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"metadata":{"id":"PArBfW8sZoJn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"b3f49449-df0a-4edc-e40c-2bbf11ab7373","executionInfo":{"status":"ok","timestamp":1537844519787,"user_tz":-480,"elapsed":3326,"user":{"displayName":"sara z","photoUrl":"","userId":"01951022893791472695"}}},"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import data\n","from datetime import datetime\n","import time\n","import multiprocessing\n","import shutil\n","import os\n","os.chdir(\"/content/drive/My Drive/lower_level_api\")\n","\n","print(tf.__version__)\n","\n","MODEL_NAME = 'estimator-demo-'+ time.strftime('%m%d%H%M%S', time.localtime())\n","\n","TRAIN_DATA_FILES_PATTERN = 'data/sms-spam/train-*.tsv'\n","VALID_DATA_FILES_PATTERN = 'data/sms-spam/valid-*.tsv'\n","\n","VOCAB_LIST_FILE = 'data/sms-spam/vocab_list.tsv'\n","N_WORDS_FILE = 'data/sms-spam/n_words.tsv'\n","\n","RESUME_TRAINING = False\n","MULTI_THREADING = True\n","\n","# dataset metadata\n","MAX_DOCUMENT_LENGTH = 50\n","\n","PAD_WORD = '#=KS=#'\n","\n","HEADER = ['class', 'sms']\n","HEADER_DEFAULTS = [['NA'], ['NA']]\n","\n","TEXT_FEATURE_NAME = 'sms'\n","\n","TARGET_NAME = 'class'\n","\n","WEIGHT_COLUNM_NAME = 'weight'\n","\n","TARGET_LABELS = ['spam', 'ham']\n","\n","with open(N_WORDS_FILE) as file:\n","    N_WORDS = int(file.read())+2\n","print(N_WORDS)\n","\n","# data input func\n","\n","def parse_tsv_row(tsv_row):\n","    \n","    columns = tf.decode_csv(tsv_row, record_defaults=HEADER_DEFAULTS, field_delim='\\t')\n","    features = dict(zip(HEADER, columns))\n","    \n","    target = features.pop(TARGET_NAME)\n","    \n","    # giving more weight to \"spam\" records are the are only 13% of the training set\n","    features[WEIGHT_COLUNM_NAME] =  tf.cond( tf.equal(target,'spam'), lambda: 6.6, lambda: 1.0 ) \n","\n","    return features, target\n","  \n","def parse_label_column(label_string_tensor):\n","    table = tf.contrib.lookup.index_table_from_tensor(tf.constant(TARGET_LABELS))\n","    return table.lookup(label_string_tensor)\n","\n","def input_fn(files_name_pattern, mode=tf.estimator.ModeKeys.EVAL, \n","                 skip_header_lines=0, \n","                 num_epochs=1, \n","                 batch_size=200):\n","    \n","    shuffle = True if mode == tf.estimator.ModeKeys.TRAIN else False\n","    \n","    num_threads = multiprocessing.cpu_count() if MULTI_THREADING else 1\n","    \n","    buffer_size = 2 * batch_size + 1\n","   \n","    print(\"\")\n","    print(\"* data input_fn:\")\n","    print(\"================\")\n","    print(\"Input file(s): {}\".format(files_name_pattern))\n","    print(\"Batch size: {}\".format(batch_size))\n","    print(\"Epoch Count: {}\".format(num_epochs))\n","    print(\"Mode: {}\".format(mode))\n","    print(\"Thread Count: {}\".format(num_threads))\n","    print(\"Shuffle: {}\".format(shuffle))\n","    print(\"================\")\n","    print(\"\")\n","\n","    file_names = tf.matching_files(files_name_pattern)\n","    dataset = data.TextLineDataset(filenames=file_names)\n","    \n","    dataset = dataset.skip(skip_header_lines)\n","    \n","    if shuffle:\n","        dataset = dataset.shuffle(buffer_size)\n","        \n","    dataset = dataset.map(lambda tsv_row: parse_tsv_row(tsv_row), \n","                          num_parallel_calls=num_threads)\n","    \n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.repeat(num_epochs)\n","    dataset = dataset.prefetch(buffer_size)\n","    \n","    iterator = dataset.make_one_shot_iterator()\n","    \n","    features, target = iterator.get_next()\n","    return features, parse_label_column(target)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["1.11.0-rc1\n","11332\n"],"name":"stdout"}]},{"metadata":{"id":"VljvlKW4ZoJ3","colab_type":"text"},"cell_type":"markdown","source":["## 3. Define Model Function"]},{"metadata":{"id":"etSDDJw_ZoJ3","colab_type":"code","colab":{}},"cell_type":"code","source":["def process_text(text_feature):\n","    \n","    # Load vocabolary lookup table to map word => word_id\n","    vocab_table = tf.contrib.lookup.index_table_from_file(vocabulary_file=VOCAB_LIST_FILE, \n","                                                          num_oov_buckets=1, default_value=-1)\n","    # Get text feature\n","    smss = text_feature\n","    # Split text to words -> this will produce sparse tensor with variable-lengthes (word count) entries\n","    words = tf.string_split(smss)\n","    # Convert sparse tensor to dense tensor by padding each entry to match the longest in the batch\n","    dense_words = tf.sparse_tensor_to_dense(words, default_value=PAD_WORD)\n","    # Convert word to word_ids via the vocab lookup table\n","    word_ids = vocab_table.lookup(dense_words)\n","    # Create a word_ids padding\n","    padding = tf.constant([[0,0],[0,MAX_DOCUMENT_LENGTH]])\n","    # Pad all the word_ids entries to the maximum document length\n","    word_ids_padded = tf.pad(word_ids, padding)\n","    word_id_vector = tf.slice(word_ids_padded, [0,0], [-1, MAX_DOCUMENT_LENGTH])\n","    \n","    # Return the final word_id_vector\n","    return word_id_vector\n","\n","\n","def model_fn(features, labels, mode, params):\n","    \n","    hidden_units = params.hidden_units\n","    output_layer_size = len(TARGET_LABELS)\n","    embedding_size = params.embedding_size\n","    forget_bias = params.forget_bias\n","    keep_prob = params.keep_prob\n","    \n","    # word_id_vector\n","    word_id_vector = process_text(features[TEXT_FEATURE_NAME]) \n","    # print(\"word_id_vector: {}\".format(word_id_vector)) # (?, MAX_DOCUMENT_LENGTH)\n","    \n","    # layer to take each word_id and convert it into vector (embeddings) \n","    word_embeddings = tf.contrib.layers.embed_sequence(word_id_vector, vocab_size=N_WORDS, \n","                                                 embed_dim=embedding_size) \n","    #print(\"word_embeddings: {}\".format(word_embeddings)) # (?, MAX_DOCUMENT_LENGTH, embbeding_size)\n","    \n","    # configure the RNN\n","    rnn_layers = [tf.nn.rnn_cell.LSTMCell(\n","        num_units=size, \n","        forget_bias=params.forget_bias,\n","        activation=tf.nn.tanh) for size in hparams.hidden_units]\n","\n","    # create a RNN cell composed sequentially of a number of RNNCells\n","    multi_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(rnn_layers)\n","    \n","    input_layer = tf.unstack(word_embeddings, axis=1)\n","    # list of len(MAX_DOCUMENT_LENGTH), each element is (?,  embbeding_size)\n","    #print(\"input_layer: {}\".format(input_layer)) \n","    \n","    outputs, _ = tf.nn.static_rnn(cell=multi_rnn_cell, \n","                                inputs=input_layer, \n","                                dtype=tf.float32)\n","    \n","    # slice to keep only the last cell of the RNN\n","    rnn_output = outputs[-1]\n","\n","    # Connect the output layer (logits) to the hidden layer (no activation fn)\n","    logits = tf.layers.dense(inputs=rnn_output, \n","                             units=output_layer_size, \n","                             activation=None)\n","    # print(\"logits: {}\".format(logits)) # (?, output_layer_size)\n","\n","    # Provide an estimator spec for `ModeKeys.PREDICT`.\n","    if mode == tf.estimator.ModeKeys.PREDICT:\n","        probabilities = tf.nn.softmax(logits)\n","        predicted_indices = tf.argmax(probabilities, 1)\n","\n","        # Convert predicted_indices back into strings\n","        predictions = {\n","            'class': tf.gather(TARGET_LABELS, predicted_indices),\n","            'probabilities': probabilities\n","        }\n","        export_outputs = {\n","            'prediction': tf.estimator.export.PredictOutput(predictions)\n","        }\n","        \n","        # Provide an estimator spec for `ModeKeys.PREDICT` modes.\n","        return tf.estimator.EstimatorSpec(mode,\n","                                          predictions=predictions,\n","                                          export_outputs=export_outputs)\n","    \n","    # weights\n","    weights = features[WEIGHT_COLUNM_NAME]\n","\n","    # Calculate loss using softmax cross entropy\n","    loss = tf.losses.sparse_softmax_cross_entropy(\n","        logits=logits, labels=labels, \n","        weights=weights\n","    )\n","    \n","    tf.summary.scalar('loss', loss)\n","    \n","    if mode == tf.estimator.ModeKeys.TRAIN:\n","        # Create Optimiser\n","        optimizer = tf.train.AdamOptimizer(params.learning_rate)\n","\n","        # Create training operation\n","        train_op = optimizer.minimize(\n","            loss=loss, global_step=tf.train.get_global_step())\n","\n","        # Provide an estimator spec for `ModeKeys.TRAIN` modes.\n","        return tf.estimator.EstimatorSpec(mode=mode,\n","                                          loss=loss, \n","                                          train_op=train_op)\n","        \n","\n","    if mode == tf.estimator.ModeKeys.EVAL:\n","        probabilities = tf.nn.softmax(logits)\n","        predicted_indices = tf.argmax(probabilities, 1)\n","\n","        # Return accuracy and area under ROC curve metrics\n","        labels_one_hot = tf.one_hot(\n","            labels,\n","            depth=len(TARGET_LABELS),\n","            on_value=True,\n","            off_value=False,\n","            dtype=tf.bool\n","        )\n","        \n","        eval_metric_ops = {\n","            'accuracy': tf.metrics.accuracy(labels, predicted_indices, weights=weights),\n","            'auroc': tf.metrics.auc(labels_one_hot, probabilities, weights=weights)\n","        }\n","        \n","        # Provide an estimator spec for `ModeKeys.EVAL` modes.\n","        return tf.estimator.EstimatorSpec(mode, \n","                                          loss=loss, \n","                                          eval_metric_ops=eval_metric_ops)\n","\n","def create_estimator(run_config, hparams):\n","    estimator = tf.estimator.Estimator(model_fn=model_fn, \n","                                  params=hparams, \n","                                  config=run_config)\n","    \n","    print(\"\")\n","    print(\"Estimator Type: {}\".format(type(estimator)))\n","    print(\"\")\n","\n","    return estimator"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Fv50wBo4ZoJ5","colab_type":"text"},"cell_type":"markdown","source":["## 4. Run Experiment"]},{"metadata":{"id":"O7zFlc8jZoJ6","colab_type":"text"},"cell_type":"markdown","source":["### a. Set HParam and RunConfig"]},{"metadata":{"id":"ZRd5MGWoZoJ7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":173},"outputId":"0a6076f9-6b4b-4d7d-c9d6-d1cda108a9b7","executionInfo":{"status":"ok","timestamp":1537845538669,"user_tz":-480,"elapsed":2310,"user":{"displayName":"sara z","photoUrl":"","userId":"01951022893791472695"}}},"cell_type":"code","source":["TRAIN_SIZE = 4179\n","NUM_EPOCHS = 100\n","BATCH_SIZE = 250\n","EVAL_AFTER_SEC = 60\n","TOTAL_STEPS = int((TRAIN_SIZE/BATCH_SIZE)*NUM_EPOCHS)\n","\n","hparams  = tf.contrib.training.HParams(\n","    num_epochs = NUM_EPOCHS,\n","    batch_size = BATCH_SIZE,\n","    embedding_size = 200,\n","    forget_bias=1.0,\n","    keep_prob = 0.8,\n","    hidden_units=[24, 16],\n","    max_steps = TOTAL_STEPS,\n","    learning_rate = 0.01\n",")\n","\n","model_dir = 'trained_models/{}'.format(MODEL_NAME)\n","\n","run_config = tf.estimator.RunConfig(\n","    log_step_count_steps=5000,\n","    tf_random_seed=19830610,\n","    model_dir=model_dir\n",")\n","\n","print(hparams)\n","print(\"Model Directory:\", run_config.model_dir)\n","print(\"\")\n","print(\"Dataset Size:\", TRAIN_SIZE)\n","print(\"Batch Size:\", BATCH_SIZE)\n","print(\"Steps per Epoch:\",TRAIN_SIZE/BATCH_SIZE)\n","print(\"Total Steps:\", TOTAL_STEPS)\n","print(\"That is 1 evaluation step after each\",EVAL_AFTER_SEC,\"training seconds\")"],"execution_count":33,"outputs":[{"output_type":"stream","text":["[('batch_size', 250), ('embedding_size', 200), ('forget_bias', 1.0), ('hidden_units', [24, 16]), ('keep_prob', 0.8), ('learning_rate', 0.01), ('max_steps', 1671), ('num_epochs', 100)]\n","Model Directory: trained_models/estimator-demo-0925030157\n","\n","Dataset Size: 4179\n","Batch Size: 250\n","Steps per Epoch: 16.716\n","Total Steps: 1671\n","That is 1 evaluation step after each 60 training seconds\n"],"name":"stdout"}]},{"metadata":{"id":"L8KQqrHIZoJ-","colab_type":"code","colab":{}},"cell_type":"code","source":["def serving_input_fn():\n","    \n","    receiver_tensor = {\n","      'sms': tf.placeholder(tf.string, [None]),\n","    }\n","    \n","    features = {\n","      key: tensor\n","      for key, tensor in receiver_tensor.items()\n","    }\n","    \n","    return tf.estimator.export.ServingInputReceiver(\n","        features, receiver_tensor)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JlH7c6SvZoKB","colab_type":"code","colab":{}},"cell_type":"code","source":["train_spec = tf.estimator.TrainSpec(\n","    input_fn = lambda: input_fn(\n","        TRAIN_DATA_FILES_PATTERN,\n","        mode = tf.estimator.ModeKeys.TRAIN,\n","        num_epochs=hparams.num_epochs,\n","        batch_size=hparams.batch_size\n","    ),\n","    max_steps=hparams.max_steps,\n","    hooks=None\n",")\n","\n","eval_spec = tf.estimator.EvalSpec(\n","    input_fn = lambda: input_fn(\n","        VALID_DATA_FILES_PATTERN,\n","        mode=tf.estimator.ModeKeys.EVAL,\n","        batch_size=hparams.batch_size\n","    ),\n","    exporters=[tf.estimator.LatestExporter(\n","        name=\"predict\", # the name of the folder in which the model will be exported to under export\n","        serving_input_receiver_fn=serving_input_fn,\n","        exports_to_keep=1,\n","        as_text=True)],\n","    steps=None,\n","    throttle_secs = EVAL_AFTER_SEC\n",")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vQjXcgmwZoKD","colab_type":"text"},"cell_type":"markdown","source":["### d. Run Experiment via train_and_evaluate"]},{"metadata":{"id":"EEVtsyDHZoKE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1261},"outputId":"c5467bd4-2f51-402e-85de-f3e3387e4156","executionInfo":{"status":"ok","timestamp":1537845704436,"user_tz":-480,"elapsed":162091,"user":{"displayName":"sara z","photoUrl":"","userId":"01951022893791472695"}}},"cell_type":"code","source":["if not RESUME_TRAINING:\n","    print(\"Removing previous artifacts...\")\n","    shutil.rmtree(model_dir, ignore_errors=True)\n","else:\n","    print(\"Resuming training...\") \n","\n","    \n","tf.logging.set_verbosity(tf.logging.INFO)\n","\n","time_start = datetime.utcnow() \n","print(\"Experiment started at {}\".format(time_start.strftime(\"%H:%M:%S\")))\n","print(\".......................................\") \n","\n","estimator = create_estimator(run_config, hparams)\n","\n","tf.estimator.train_and_evaluate(\n","    estimator=estimator,\n","    train_spec=train_spec, \n","    eval_spec=eval_spec\n",")\n","\n","time_end = datetime.utcnow() \n","print(\".......................................\")\n","print(\"Experiment finished at {}\".format(time_end.strftime(\"%H:%M:%S\")))\n","print(\"\")\n","time_elapsed = time_end - time_start\n","print(\"Experiment elapsed time: {} seconds\".format(time_elapsed.total_seconds()))\n","    "],"execution_count":36,"outputs":[{"output_type":"stream","text":["Removing previous artifacts...\n","Experiment started at 03:19:03\n",".......................................\n","INFO:tensorflow:Using config: {'_model_dir': 'trained_models/estimator-demo-0925030157', '_tf_random_seed': 19830610, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 5000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff47bef1fd0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n","\n","Estimator Type: <class 'tensorflow.python.estimator.estimator.Estimator'>\n","\n","INFO:tensorflow:Running training and evaluation locally (non-distributed).\n","INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n","\n","* data input_fn:\n","================\n","Input file(s): data/sms-spam/train-*.tsv\n","Batch size: 250\n","Epoch Count: 100\n","Mode: train\n","Thread Count: 2\n","Shuffle: True\n","================\n","\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Create CheckpointSaverHook.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Saving checkpoints for 0 into trained_models/estimator-demo-0925030157/model.ckpt.\n","INFO:tensorflow:loss = 1.2211835, step = 0\n","INFO:tensorflow:Saving checkpoints for 1671 into trained_models/estimator-demo-0925030157/model.ckpt.\n","\n","* data input_fn:\n","================\n","Input file(s): data/sms-spam/valid-*.tsv\n","Batch size: 250\n","Epoch Count: 1\n","Mode: eval\n","Thread Count: 2\n","Shuffle: False\n","================\n","\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Starting evaluation at 2018-09-25-03:21:40\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from trained_models/estimator-demo-0925030157/model.ckpt-1671\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Finished evaluation at 2018-09-25-03:21:42\n","INFO:tensorflow:Saving dict for global step 1671: accuracy = 0.96351266, auroc = 0.96465296, global_step = 1671, loss = 0.67617995\n","INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1671: trained_models/estimator-demo-0925030157/model.ckpt-1671\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n","INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n","INFO:tensorflow:Signatures INCLUDED in export for Predict: ['prediction', 'serving_default']\n","INFO:tensorflow:Signatures INCLUDED in export for Train: None\n","INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n","INFO:tensorflow:Restoring parameters from trained_models/estimator-demo-0925030157/model.ckpt-1671\n","INFO:tensorflow:Assets added to graph.\n","INFO:tensorflow:Assets written to: trained_models/estimator-demo-0925030157/export/predict/temp-b'1537845702'/assets\n","INFO:tensorflow:SavedModel written to: trained_models/estimator-demo-0925030157/export/predict/temp-b'1537845702'/saved_model.pbtxt\n","INFO:tensorflow:Loss for final step: 1.5588394e-05.\n",".......................................\n","Experiment finished at 03:21:44\n","\n","Experiment elapsed time: 161.144033 seconds\n"],"name":"stdout"}]},{"metadata":{"id":"CkOqpY_FZoKH","colab_type":"text"},"cell_type":"markdown","source":["## 5. Evaluate the Model"]},{"metadata":{"id":"cBOpFObUZoKI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1091},"outputId":"cb4607f8-7ee2-45cf-d367-7c3865423ad4","executionInfo":{"status":"ok","timestamp":1537845713298,"user_tz":-480,"elapsed":8816,"user":{"displayName":"sara z","photoUrl":"","userId":"01951022893791472695"}}},"cell_type":"code","source":["TRAIN_SIZE = 4179\n","TEST_SIZE = 1393\n","\n","train_input_fn = lambda: input_fn(files_name_pattern= TRAIN_DATA_FILES_PATTERN, \n","                                      mode= tf.estimator.ModeKeys.EVAL,\n","                                      batch_size= TRAIN_SIZE)\n","\n","test_input_fn = lambda: input_fn(files_name_pattern= VALID_DATA_FILES_PATTERN, \n","                                      mode= tf.estimator.ModeKeys.EVAL,\n","                                      batch_size= TEST_SIZE)\n","\n","estimator = create_estimator(run_config, hparams)\n","\n","train_results = estimator.evaluate(input_fn=train_input_fn, steps=1)\n","print()\n","print(\"######################################################################################\")\n","print(\"# Train Measures: {}\".format(train_results))\n","print(\"######################################################################################\")\n","\n","test_results = estimator.evaluate(input_fn=test_input_fn, steps=1)\n","print()\n","print(\"######################################################################################\")\n","print(\"# Test Measures: {}\".format(test_results))\n","print(\"######################################################################################\")"],"execution_count":37,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Using config: {'_model_dir': 'trained_models/estimator-demo-0925030157', '_tf_random_seed': 19830610, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 5000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff47bef1898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n","\n","Estimator Type: <class 'tensorflow.python.estimator.estimator.Estimator'>\n","\n","\n","* data input_fn:\n","================\n","Input file(s): data/sms-spam/train-*.tsv\n","Batch size: 4179\n","Epoch Count: 1\n","Mode: eval\n","Thread Count: 2\n","Shuffle: False\n","================\n","\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Starting evaluation at 2018-09-25-03:21:47\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from trained_models/estimator-demo-0925030157/model.ckpt-1671\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Evaluation [1/1]\n","INFO:tensorflow:Finished evaluation at 2018-09-25-03:21:49\n","INFO:tensorflow:Saving dict for global step 1671: accuracy = 1.0, auroc = 1.0, global_step = 1671, loss = 1.8053086e-05\n","INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1671: trained_models/estimator-demo-0925030157/model.ckpt-1671\n","\n","######################################################################################\n","# Train Measures: {'accuracy': 1.0, 'auroc': 1.0, 'loss': 1.8053086e-05, 'global_step': 1671}\n","######################################################################################\n","\n","* data input_fn:\n","================\n","Input file(s): data/sms-spam/valid-*.tsv\n","Batch size: 1393\n","Epoch Count: 1\n","Mode: eval\n","Thread Count: 2\n","Shuffle: False\n","================\n","\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Starting evaluation at 2018-09-25-03:21:51\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from trained_models/estimator-demo-0925030157/model.ckpt-1671\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Evaluation [1/1]\n","INFO:tensorflow:Finished evaluation at 2018-09-25-03:21:52\n","INFO:tensorflow:Saving dict for global step 1671: accuracy = 0.9635126, auroc = 0.96465296, global_step = 1671, loss = 0.67763937\n","INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1671: trained_models/estimator-demo-0925030157/model.ckpt-1671\n","\n","######################################################################################\n","# Test Measures: {'accuracy': 0.9635126, 'auroc': 0.96465296, 'loss': 0.67763937, 'global_step': 1671}\n","######################################################################################\n"],"name":"stdout"}]},{"metadata":{"id":"RN6TmnddZoKM","colab_type":"text"},"cell_type":"markdown","source":["## 6. Predict Using Serving Function"]},{"metadata":{"id":"Y0QgorkqZoKM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":139},"outputId":"5c1254c0-ecf4-4372-8f5f-46316874f28c","executionInfo":{"status":"ok","timestamp":1537845718406,"user_tz":-480,"elapsed":5072,"user":{"displayName":"sara z","photoUrl":"","userId":"01951022893791472695"}}},"cell_type":"code","source":["import os\n","\n","export_dir = model_dir +\"/export/predict/\"\n","\n","saved_model_dir = export_dir + \"/\" + os.listdir(path=export_dir)[-1] \n","\n","print(saved_model_dir)\n","print(\"\")\n","\n","predictor_fn = tf.contrib.predictor.from_saved_model(\n","    export_dir = saved_model_dir,\n","    signature_def_key=\"prediction\"\n",")\n","\n","output = predictor_fn(\n","    {\n","        'sms':[\n","            'ok, I will be with you in 5 min. see you then',\n","            'win 1000 cash free of charge promo hot deal sexy',\n","            'hot girls sexy tonight call girls waiting for chat'\n","        ]\n","        \n","    }\n",")\n","print(output)"],"execution_count":38,"outputs":[{"output_type":"stream","text":["trained_models/estimator-demo-0925030157/export/predict//1537845702\n","\n","INFO:tensorflow:Restoring parameters from trained_models/estimator-demo-0925030157/export/predict//1537845702/variables/variables\n","{'class': array([b'ham', b'spam', b'spam'], dtype=object), 'probabilities': array([[8.4779622e-06, 9.9999154e-01],\n","       [9.9998772e-01, 1.2297025e-05],\n","       [9.9998629e-01, 1.3683886e-05]], dtype=float32)}\n"],"name":"stdout"}]},{"metadata":{"id":"Bk8UQUlht4Rr","colab_type":"text"},"cell_type":"markdown","source":["# Understanding memory and time usage"]},{"metadata":{"id":"Gj5I0uZxsN5z","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":425},"outputId":"fa459284-3901-4066-d413-1c93f2513345","executionInfo":{"status":"ok","timestamp":1537845719918,"user_tz":-480,"elapsed":1485,"user":{"displayName":"sara z","photoUrl":"","userId":"01951022893791472695"}}},"cell_type":"code","source":["import numpy as np\n","\n","variables = estimator.get_variable_names()\n","for var_name in variables:\n","  print(var_name, \": \", estimator.get_variable_value(var_name).shape)"],"execution_count":39,"outputs":[{"output_type":"stream","text":["EmbedSequence/embeddings :  (11332, 200)\n","EmbedSequence/embeddings/Adam :  (11332, 200)\n","EmbedSequence/embeddings/Adam_1 :  (11332, 200)\n","beta1_power :  ()\n","beta2_power :  ()\n","dense/bias :  (2,)\n","dense/bias/Adam :  (2,)\n","dense/bias/Adam_1 :  (2,)\n","dense/kernel :  (16, 2)\n","dense/kernel/Adam :  (16, 2)\n","dense/kernel/Adam_1 :  (16, 2)\n","global_step :  ()\n","rnn/multi_rnn_cell/cell_0/lstm_cell/bias :  (96,)\n","rnn/multi_rnn_cell/cell_0/lstm_cell/bias/Adam :  (96,)\n","rnn/multi_rnn_cell/cell_0/lstm_cell/bias/Adam_1 :  (96,)\n","rnn/multi_rnn_cell/cell_0/lstm_cell/kernel :  (224, 96)\n","rnn/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam :  (224, 96)\n","rnn/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam_1 :  (224, 96)\n","rnn/multi_rnn_cell/cell_1/lstm_cell/bias :  (64,)\n","rnn/multi_rnn_cell/cell_1/lstm_cell/bias/Adam :  (64,)\n","rnn/multi_rnn_cell/cell_1/lstm_cell/bias/Adam_1 :  (64,)\n","rnn/multi_rnn_cell/cell_1/lstm_cell/kernel :  (40, 64)\n","rnn/multi_rnn_cell/cell_1/lstm_cell/kernel/Adam :  (40, 64)\n","rnn/multi_rnn_cell/cell_1/lstm_cell/kernel/Adam_1 :  (40, 64)\n"],"name":"stdout"}]},{"metadata":{"id":"y8-Zy1ansPPK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"ec2a1bff-1d00-4d5d-a846-553f1c4b9290","executionInfo":{"status":"ok","timestamp":1537845722226,"user_tz":-480,"elapsed":2269,"user":{"displayName":"sara z","photoUrl":"","userId":"01951022893791472695"}}},"cell_type":"code","source":["param_count = np.sum([np.prod(estimator.get_variable_value(var_name).shape) for var_name in estimator.get_variable_names()])\n","param_count"],"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["6871977.0"]},"metadata":{"tags":[]},"execution_count":40}]}]}